{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIpA3NJ7SIstty0ZY2kPJr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahnavi6078/web-scrapping/blob/main/Untitled26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q-W8-9YXfCzw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Define your User-Agent string\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
        "\n",
        "# Function to scrape product details from a single page\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": user_agent\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page. Status Code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    product_details = []\n",
        "\n",
        "    products = soup.find_all(\"div\", class_=\"s-result-item\")\n",
        "    for product in products:\n",
        "        product_url_element = product.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
        "        if not product_url_element:\n",
        "            continue  # Skip this product if the URL is not found\n",
        "\n",
        "        product_url = product_url_element.get(\"href\")\n",
        "\n",
        "        product_name_element = product.find(\"span\", class_=\"a-text-normal\")\n",
        "        if not product_name_element:\n",
        "            continue  # Skip this product if the name is not found\n",
        "        product_name = product_name_element.text.strip()\n",
        "\n",
        "        product_price_element = product.find(\"span\", class_=\"a-price-whole\")\n",
        "        if product_price_element:\n",
        "            product_price = product_price_element.text\n",
        "        else:\n",
        "            product_price = \"Not available\"\n",
        "\n",
        "        product_rating_element = product.find(\"span\", class_=\"a-icon-alt\")\n",
        "        if product_rating_element:\n",
        "            product_rating = product_rating_element.text\n",
        "        else:\n",
        "            product_rating = \"Not rated\"\n",
        "\n",
        "        product_reviews_element = product.find(\"span\", {\"class\": \"a-size-base\", \"aria-label\": \" customer review\"})\n",
        "        if product_reviews_element:\n",
        "            product_reviews = product_reviews_element.text\n",
        "        else:\n",
        "            product_reviews = \"No reviews\"\n",
        "\n",
        "        product_details.append({\n",
        "            \"Product URL\": product_url,\n",
        "            \"Product Name\": product_name,\n",
        "            \"Product Price\": product_price,\n",
        "            \"Rating\": product_rating,\n",
        "            \"Number of reviews\": product_reviews\n",
        "        })\n",
        "\n",
        "    return product_details\n",
        "\n",
        "# Function to scrape data from multiple pages\n",
        "def scrape_multiple_pages(base_url, num_pages):\n",
        "    all_product_details = []\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}&page={page}\"\n",
        "        product_details = scrape_page(url)\n",
        "\n",
        "        if product_details:\n",
        "            all_product_details.extend(product_details)\n",
        "\n",
        "    return all_product_details\n",
        "\n",
        "# Define your base URL and the number of pages to scrape\n",
        "base_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2C,aps,283&ref=sr_pg_1\"\n",
        "num_pages = 20  # Set the number of pages to scrape\n",
        "\n",
        "scraped_data = scrape_multiple_pages(base_url, num_pages)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "csv_file = 'amazon_products.csv'\n",
        "\n",
        "with open(csv_file, 'w', newline='') as csvfile:\n",
        "    fieldnames = ['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of reviews']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for product in scraped_data:\n",
        "        writer.writerow(product)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "\n",
        "# Define your User-Agent string\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
        "\n",
        "# Function to scrape additional product details from a single product page\n",
        "def scrape_product_details(product_url):\n",
        "    headers = {\n",
        "        \"User-Agent\": user_agent\n",
        "    }\n",
        "\n",
        "    # Prepend 'https://' to the URL if it's a relative URL\n",
        "    if not product_url.startswith(\"http\"):\n",
        "        product_url = \"https://www.amazon.in\" + product_url\n",
        "\n",
        "    retries = 3  # Number of retries\n",
        "    for _ in range(retries):\n",
        "        response = requests.get(product_url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            break\n",
        "        else:\n",
        "            print(f\"Failed to retrieve the product page. Status Code: {response.status_code}\")\n",
        "            time.sleep(5)  # Wait for a few seconds before retrying\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to retrieve the product page after retries.\")\n",
        "        return {}\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    product_details = {\n",
        "        \"Description\": \"\",\n",
        "        \"ASIN\": \"\",\n",
        "        \"Product Description\": \"\",\n",
        "        \"Manufacturer\": \"\"\n",
        "    }\n",
        "\n",
        "    # Extract additional information here (modify as needed)\n",
        "    # Example:\n",
        "    product_title_element = soup.find(\"span\", id=\"productTitle\")\n",
        "    if product_title_element:\n",
        "        product_details[\"Description\"] = product_title_element.text.strip()\n",
        "    else:\n",
        "        product_details[\"Description\"] = \"Not available\"\n",
        "\n",
        "    asin_element = soup.find(\"th\", text=\"ASIN\")\n",
        "    if asin_element:\n",
        "        asin_td = asin_element.find_next(\"td\")\n",
        "        if asin_td:\n",
        "            product_details[\"ASIN\"] = asin_td.text.strip()\n",
        "        else:\n",
        "            product_details[\"ASIN\"] = \"Not available\"\n",
        "    else:\n",
        "        product_details[\"ASIN\"] = \"Not available\"\n",
        "\n",
        "    product_description_element = soup.find(\"div\", id=\"productDescription\")\n",
        "    if product_description_element:\n",
        "        product_details[\"Product Description\"] = product_description_element.text.strip()\n",
        "    else:\n",
        "        product_details[\"Product Description\"] = \"Not available\"\n",
        "\n",
        "    manufacturer_element = soup.find(\"th\", text=\"Manufacturer\")\n",
        "    if manufacturer_element:\n",
        "        manufacturer_td = manufacturer_element.find_next(\"td\")\n",
        "        if manufacturer_td:\n",
        "            product_details[\"Manufacturer\"] = manufacturer_td.text.strip()\n",
        "        else:\n",
        "            product_details[\"Manufacturer\"] = \"Not available\"\n",
        "    else:\n",
        "        product_details[\"Manufacturer\"] = \"Not available\"\n",
        "\n",
        "    return product_details\n",
        "\n",
        "# Function to scrape data from multiple product URLs\n",
        "def scrape_multiple_product_details(product_urls):\n",
        "    all_product_details = []\n",
        "\n",
        "    for url in product_urls:\n",
        "        product_details = scrape_product_details(url)\n",
        "\n",
        "        if product_details:\n",
        "            all_product_details.append(product_details)\n",
        "\n",
        "    return all_product_details\n",
        "\n",
        "# Load the product URLs from the CSV file created earlier\n",
        "product_urls = []\n",
        "\n",
        "with open('amazon_products.csv', 'r', newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        product_urls.append(row['Product URL'])\n",
        "\n",
        "# Scrape additional details for the specified number of product URLs\n",
        "product_details = scrape_multiple_product_details(product_urls[:200])\n",
        "\n",
        "# Save the additional data to a CSV file\n",
        "additional_data_csv = 'additional_product_data.csv'\n",
        "\n",
        "with open(additional_data_csv, 'w', newline='') as csvfile:\n",
        "    fieldnames = [\"Description\", \"ASIN\", \"Product Description\", \"Manufacturer\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for product in product_details:\n",
        "        writer.writerow(product)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz02uFYKgDLz",
        "outputId": "bd49964a-0291-4d6e-d2f7-245c4b804b6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-356f3cf92299>:50: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  asin_element = soup.find(\"th\", text=\"ASIN\")\n",
            "<ipython-input-6-356f3cf92299>:66: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  manufacturer_element = soup.find(\"th\", text=\"Manufacturer\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page after retries.\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page after retries.\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page after retries.\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page after retries.\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page after retries.\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page after retries.\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page. Status Code: 503\n",
            "Failed to retrieve the product page after retries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jahnavi6078/web-scrapping.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0lJUbJ1uHzY",
        "outputId": "eef8ce30-7257-4756-c4be-d78cc28460e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'web-scrapping'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RwJcMS_WuIYV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}